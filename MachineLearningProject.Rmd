---
title: "Machine Learning project"
author: "Joris Van den Bossche"
date: "15 september 2016"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

# Synopsis
A training and testing data set from [groupware](http://groupware.les.inf.puc-rio.br/har) is examined. The goal is to predict the classe of the cases in the test set by training a model. A random forest model and a boosted forest model are set up and their accuracies are tested with k-fold cross validation. Eventually the random forest model is chosen as the best performing model. With accuracy of 98.3%, the classes of the testing data are predicted.

# Data
The data used for this project is collected from cloudfront. More information about this data set can be found [here](http://groupware.les.inf.puc-rio.br/har), and more specifically on this data set [here](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf).
```{r, cache=TRUE}
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", stringsAsFactors = FALSE)
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", stringsAsFactors = FALSE)
```

# Data exploration
The data frame is very large, so it would be pointless to look at every variable seperately and make plots.
```{r}
dim(training)
```

The column that will be producted is the "class" column.
```{r}
table(training$classe)
```

It seems the different outcomes are somewhat evenly distributed among the data.

# Data Cleaning and preprocessing
A lot of columns are classified in the training data set as character while they are actually numerical or dates. All the character columns are set in the right format:
```{r}
for(i in c("user_name", "new_window", "classe")) training[, i] <- factor(training[, i])
training$cvtd_timestamp <- as.Date(training$cvtd_timestamp, format = "%d/%m/%Y %H:%M")
training$new_window <- ifelse(training$new_window == "yes", 1, 0)
chars <- sapply(1:160, function (x){class(training[, x]) == "character"})
for(i in which(chars)) training[, i] <- as.numeric(training[, i])
```

Before a model can be set up, good predictors must be found. A first filterring of the 159 possible predictors can be done by identifying the zero variance columns, using the function "nearZeroVar" of the caret library, and removing them from the data frame.
```{r, echo = -4}
library(caret)
nzv <- nearZeroVar(x = training)
training <- training[,-nzv]
nzv
```

Another observation is a lot of columns have many NA values. Columns with more than 90% NA are removed from the training data frame.
```{r, echo = -3}
mostlyNA <- which(data.frame(colSums(is.na(training)) > 
                               dim(training)[1]*0.9
                             )[,1])
training <- training[,-mostlyNA]
mostlyNA
```

Surprisingly, there are no more missing values in our data frame anymore after this step! It seems the columns either contained more than 90% NA values, or no NA values at all. In the next step the correlations are looked into for the numeric columns. The function "findCorrelation" will find the columns that are highly correlated with other columns so they would be best to be removed.
```{r, echo = -3}
correl <- findCorrelation(cor(training[,-c(1,2, 5,59)]), cutoff = 0.9)
training <- training[,-correl]
correl
```

After this there are still 52 columns left with possible predictors. Because of the large amount of columns, it would be a good idea to look at principal component analysis to reduce the amount of columns to make the final model less computationally heavy.
```{r, echo = -3}
set.seed(100)
preProcess <- preProcess(training, method = "pca")
preProcess
```

It will be indeed good to use pca as preprocessing, as 95% of the variance can be explained by only 23 columns.  

# Modelling
The part that is meant to be predicted is the "classe" column. Possible outcomes are numbers A through E. This means the problem is a classification problem. Supervised methods like linear regression and naive bayes will therefore be less interesting. The most promising methods would be decision trees, and boosting.  
The first model will be using decision trees on our data set. The default amount of trees used is 400, which takes a lot of computational power, so the number of trees is set to 50. The model's performance can be improved by adding more trees.
```{r, cache = TRUE}
randForModel <- train(data = training, classe ~ .,method = "rf", ntree = 50,
                      preProcess=c("pca")) # This calculation takes a few minutes!
randForModel
```
```{r}
randForModel$finalModel$confusion
```

The error rate was determined to be 0.92%, so it is a very good model! To estimate the accuracy on a test set, a crossvalidation of 10 k-folds is used.
```{r, echo = -6, cache = TRUE}
train_control <- trainControl(method="cv", number=10, savePredictions = TRUE)
randForModel <- train(data = training, classe ~ .,method = "rf", ntree = 50, 
                      preProcess=c("pca"), trControl = train_control)
library(data.table)
crossVal <- data.table(randForModel$pred)
foldsAccs <- crossVal[, sum(pred == obs)/ .N, by = Resample]
foldsAccs
```
```{r}
mean(foldsAccs$V1)
sd(foldsAccs$V1)
```

The accuracy is, with 95% certainty, above 98.3%.  

For comparison's sake, a boosting model is also fitted. Using the expand.grid function, multiple options can be explored. Different configurations are tried out using the expand.grid function.
```{r, cache = TRUE, echo = -3}
gbmGrid <-  expand.grid(interaction.depth = c(1,5,10), 
                        n.trees = (1:10)*5, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
boostModel <- train(data = training, classe ~ .,method = "gbm", 
                    verbose = FALSE, tuneGrid = gbmGrid,
                      preProcess=c("pca"), trControl = train_control)
boostModel$finalModel
```

The different configurations can be easily plotted.
```{r}
trellis.par.set(caretTheme())
plot(boostModel)
```

The accuracy goes to a maximum of 95% for a tree depth of 10 with 50 trees. The rising curve means it will probably be even better with more trees and more tree depth. This can be further analysed and tested by altering the gbmGrid variable. Another possibility would be to combine the two models. This would require to check where they (dis)agree and modelling the decisions with for example another decision tree. However, for lack of computational resources and time, this will be skipped and predictions will be made with the random forest function.  

# Making predictions
Using the random forest function, new predictions can be made from the testing data. Before applying the model, the testing data has to go through the same pre-processing. In this case, is is only removing columns. The pca transformation is present in the model.
```{r}
for(i in c("user_name", "new_window", "problem_id")) testing[, i] <- factor(testing[, i])
testing$cvtd_timestamp <- as.Date(testing$cvtd_timestamp, format = "%d/%m/%Y %H:%M")
testing$new_window <- ifelse(testing$new_window == "yes", 1, 0)
chars <- sapply(1:160, function (x){class(testing[, x]) == "character"})
for(i in which(chars)) testing[, i] <- as.numeric(testing[, i])
testing <- testing[, -nzv]
testing <- testing[, -mostlyNA]
testing <- testing[, -correl]
```

Now predictions can be made, with accuracy of 98.3%.
```{r, echo = -2}
predictions <- data.frame(Case = testing$X, prediction = predict(randForModel, testing))
predictions
```

# Final words
Thank you for reading this project assignment and have a nice day!